{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling with VAE - from tensorflow / tabular data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data and change columns names\n",
    "data = pd.read_excel('final.xlsm', header=None)\n",
    "data.columns =['Strain_ID', 'Genome_length', 'Ratio_1kbp_bins_covered', 'Ratio_GC-content_uncovered_bins', 'Ratio_N-content_uncovered_bins', 'Reads_mapped', 'Total_reads', 'Class']\n",
    "# 2. Drop the Strain_ID column\n",
    "data = data.drop(['Strain_ID'], axis=1)\n",
    "# 3. Normalize columns except Class col\n",
    "sc = StandardScaler()\n",
    "data[['Genome_length', 'Ratio_1kbp_bins_covered', 'Ratio_GC-content_uncovered_bins', 'Ratio_N-content_uncovered_bins', 'Reads_mapped', 'Total_reads']] = sc.fit_transform(data[['Genome_length', 'Ratio_1kbp_bins_covered', 'Ratio_GC-content_uncovered_bins', 'Ratio_N-content_uncovered_bins', 'Reads_mapped', 'Total_reads']])\n",
    "\n",
    "\n",
    "# 1. Prepare data: \n",
    "def prepare_data(data,random_state=12):\n",
    "    X = data.drop('Class', axis=1)\n",
    "    y = data['Class']\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    dftrain = [xtrain, ytrain]\n",
    "    dftest = [xtest, ytest]\n",
    "    return pd.concat(dftrain, axis=1), pd.concat(dftest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to obtain 2 dataframes: \n",
    "dftrain, dftest = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12337ad30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['Class']) # initial distribution of data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Strains function: \n",
    "def strain(dftrain, dftest):\n",
    "    ''' 1. select only Class = 1 values '''\n",
    "    strain = dftrain[dftrain.Class==1].sample(frac=1)\n",
    "    ''' 2. drop class from the dataset '''\n",
    "    strain = strain.drop(['Class'], axis=1)\n",
    "    straintest = dftest[dftest.Class==1].sample(frac=1)\n",
    "    straintest = straintest.drop(['Class'], axis=1)\n",
    "    return strain, straintest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Reshape\n",
    "from tensorflow.keras import Sequential\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from Tensorflow tutorial on VAEs. It's turned from a CNN to a simple neuralnet which is more appropriate for our case here.\n",
    "\n",
    "# almost the standard for activation these days\n",
    "relu = tf.nn.relu\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A VAE class inhereted from keras.Model\n",
    "\n",
    "    parameters:\n",
    "    ndim (int): number of dimensions of the input data\n",
    "    latent_dim (int): number of dimensions of the latent variable\n",
    "\n",
    "    attributes:\n",
    "    ndim (int): number of dimensions of the input\n",
    "    latent_dim (int): number of dimensions of the latent variable\n",
    "    inference_net (keras.Sequential): The inference model that takes an input of size=(None, ndim) and return a matrix of size=(None, latent_dim)\n",
    "    generative_net (keras.Sequential): The generative model that takes an input of size=(None, latent_dim) and return a matrix of size=(None, ndim)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim  \n",
    "        self.ndim = ndim        \n",
    "        self.inference_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(ndim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(2 * latent_dim)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.generative_net = Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(latent_dim,)),\n",
    "                Dense(100, activation=relu),\n",
    "                Dense(ndim)\n",
    "            ])\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, num_samples=100, eps=None):\n",
    "        \"\"\"\n",
    "        Given an input noise of size (num_samples, latent_dim), generate samples of size (num_samples, ndim)\n",
    "\n",
    "        parameters:\n",
    "        num_samples (int): number of samples\n",
    "        eps (numpy.ndarray): input noise. if specified, num_samples is ignored\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the decoded samples\n",
    "        \"\"\"\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(num_samples, self.latent_dim))\n",
    "        return self.decode(eps)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters:\n",
    "        x (numpy.ndarray): the input data with size (None, ndim)\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the mean of the latent variables\n",
    "        numpy.ndarray: the log variance of the latent variables\n",
    "        \"\"\"\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterize the input for backpropagation\n",
    "\n",
    "        parameters:\n",
    "        mean (numpy.ndarray): the mean of the latent variables\n",
    "        logvar (numpy.ndarray): the log variance of the latent variables\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the noise samples from a normal distribution around mean with standard deviation exp(logvar / 2)\n",
    "        \"\"\"\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Given an input noise generates the decoded samples\n",
    "\n",
    "        parameters:\n",
    "        z (numpy.ndarray): the input noise (None, latent_dim)\n",
    "\n",
    "        returns:\n",
    "        numpy.ndarray: the decoded samples of size (None, ndim)\n",
    "        \"\"\"\n",
    "        return self.generative_net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for computing the KL term of Gaussian distribution\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-0.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "                         axis=raxis)\n",
    "\n",
    "# a function to compute the loss of the VAE\n",
    "@tf.function\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    logvar = tf.clip_by_value(logvar, -88., 88.)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    xmean = model.decode(z)\n",
    "    logpx_z = -tf.reduce_sum((x - xmean) ** 2, axis=1)  # ad-hoc l2 loss that is pretty close to log-prob of a gaussian distribution withtout taking into account the variance\n",
    "    logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "# A function that given the model computes the loss, the gradients and apply the parameter update\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xtrain, xtest, model=None, load=False, filepath=None):\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "    epochs = 2000\n",
    "    latent_dim = 6 # number of our columns\n",
    "    num_train, ndim = xtrain.shape\n",
    "    num_test, _ = xtest.shape\n",
    "    if model is None:\n",
    "        model = VAE(ndim, latent_dim)\n",
    "    if load and filepath is not None:\n",
    "        model.load_weights(filepath=filepath)\n",
    "        return model\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(xtrain.values.astype(np.float32)).shuffle(num_train).batch(\n",
    "            batch_size)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(xtest.values.astype(np.float32)).shuffle(num_test).batch(num_test)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            for train_x in train_dataset:\n",
    "                compute_apply_gradients(model, train_x, optimizer)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = tf.keras.metrics.Mean()\n",
    "                for test_x in test_dataset:\n",
    "                    loss(compute_loss(model, test_x))\n",
    "                elbo = -loss.result()\n",
    "                print('Epoch: {}, Test set psudo-ELBO: {}, '\n",
    "                      'time elapse for current epoch {}'.format(epoch, elbo, end_time - start_time))\n",
    "                model.save_weights('saved_models/model_%d_at_%d' % (latent_dim, epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to increase the 1 of Class \n",
    "def increasing(data, model):\n",
    "    np.random.seed(12)\n",
    "    ''' num_samples = number of values of majority class (0) '''\n",
    "    num_samples = num_samples = data['Class'].value_counts()[0] - data['Class'].value_counts()[1]  # values of class == 1 \n",
    "    samples = model.sample(num_samples=num_samples).numpy()\n",
    "    ''' creating new dataframe with sampled values '''\n",
    "    dfnew = pd.DataFrame(samples, columns=data.columns.drop('Class'))\n",
    "    dfnew['Class'] = np.ones(len(samples), dtype=np.int)\n",
    "    # dfnew = pd.concat((data, dfnew), ignore_index=True).sample(frac=1)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Test set psudo-ELBO: -7.884058475494385, time elapse for current epoch 0.009447097778320312\n",
      "Epoch: 200, Test set psudo-ELBO: -7.654962062835693, time elapse for current epoch 0.009231805801391602\n",
      "Epoch: 300, Test set psudo-ELBO: -7.074985980987549, time elapse for current epoch 0.011126995086669922\n",
      "Epoch: 400, Test set psudo-ELBO: -6.751537322998047, time elapse for current epoch 0.009316205978393555\n",
      "Epoch: 500, Test set psudo-ELBO: -6.851338863372803, time elapse for current epoch 0.008263349533081055\n",
      "Epoch: 600, Test set psudo-ELBO: -6.584784030914307, time elapse for current epoch 0.018030881881713867\n",
      "Epoch: 700, Test set psudo-ELBO: -6.305478572845459, time elapse for current epoch 0.010635137557983398\n",
      "Epoch: 800, Test set psudo-ELBO: -6.041736125946045, time elapse for current epoch 0.013313055038452148\n",
      "Epoch: 900, Test set psudo-ELBO: -5.810840606689453, time elapse for current epoch 0.00789785385131836\n",
      "Epoch: 1000, Test set psudo-ELBO: -5.844872951507568, time elapse for current epoch 0.007877111434936523\n",
      "Epoch: 1100, Test set psudo-ELBO: -5.984947681427002, time elapse for current epoch 0.012613296508789062\n",
      "Epoch: 1200, Test set psudo-ELBO: -5.6716766357421875, time elapse for current epoch 0.008004903793334961\n",
      "Epoch: 1300, Test set psudo-ELBO: -5.554898738861084, time elapse for current epoch 0.008073806762695312\n",
      "Epoch: 1400, Test set psudo-ELBO: -5.940219879150391, time elapse for current epoch 0.009072065353393555\n",
      "Epoch: 1500, Test set psudo-ELBO: -5.731597900390625, time elapse for current epoch 0.00869607925415039\n",
      "Epoch: 1600, Test set psudo-ELBO: -5.499889373779297, time elapse for current epoch 0.008804082870483398\n",
      "Epoch: 1700, Test set psudo-ELBO: -5.36591100692749, time elapse for current epoch 0.008101224899291992\n",
      "Epoch: 1800, Test set psudo-ELBO: -5.262110233306885, time elapse for current epoch 0.007884025573730469\n",
      "Epoch: 1900, Test set psudo-ELBO: -5.216376304626465, time elapse for current epoch 0.01127171516418457\n",
      "Epoch: 2000, Test set psudo-ELBO: -5.268119812011719, time elapse for current epoch 0.008679389953613281\n"
     ]
    }
   ],
   "source": [
    "# Use the function to obtain 2 dataframes: \n",
    "dftrain, dftest = prepare_data(data)\n",
    "# get the strains'presence distribution \n",
    "strain, straintest = strain(dftrain, dftest)\n",
    "# get the traied VAE model\n",
    "model = train(strain, straintest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the data using the VAE model\n",
    "data_increased = increasing(dftrain, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genome_length</th>\n",
       "      <th>Ratio_1kbp_bins_covered</th>\n",
       "      <th>Ratio_GC-content_uncovered_bins</th>\n",
       "      <th>Ratio_N-content_uncovered_bins</th>\n",
       "      <th>Reads_mapped</th>\n",
       "      <th>Total_reads</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016082</td>\n",
       "      <td>-0.06098</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>-0.060841</td>\n",
       "      <td>-0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405132</td>\n",
       "      <td>1.53619</td>\n",
       "      <td>-0.416753</td>\n",
       "      <td>-0.077574</td>\n",
       "      <td>1.532687</td>\n",
       "      <td>0.143355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Genome_length  Ratio_1kbp_bins_covered  \\\n",
       "Class                                           \n",
       "0          -0.016082                 -0.06098   \n",
       "1           0.405132                  1.53619   \n",
       "\n",
       "       Ratio_GC-content_uncovered_bins  Ratio_N-content_uncovered_bins  \\\n",
       "Class                                                                    \n",
       "0                             0.016543                        0.003079   \n",
       "1                            -0.416753                       -0.077574   \n",
       "\n",
       "       Reads_mapped  Total_reads  \n",
       "Class                             \n",
       "0         -0.060841    -0.005691  \n",
       "1          1.532687     0.143355  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class').mean() #1.53619 and 1.532687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genome_length</th>\n",
       "      <th>Ratio_1kbp_bins_covered</th>\n",
       "      <th>Ratio_GC-content_uncovered_bins</th>\n",
       "      <th>Ratio_N-content_uncovered_bins</th>\n",
       "      <th>Reads_mapped</th>\n",
       "      <th>Total_reads</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.301065</td>\n",
       "      <td>1.616199</td>\n",
       "      <td>-0.464017</td>\n",
       "      <td>-0.154447</td>\n",
       "      <td>1.491072</td>\n",
       "      <td>0.186285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Genome_length  Ratio_1kbp_bins_covered  \\\n",
       "Class                                           \n",
       "1           0.301065                 1.616199   \n",
       "\n",
       "       Ratio_GC-content_uncovered_bins  Ratio_N-content_uncovered_bins  \\\n",
       "Class                                                                    \n",
       "1                            -0.464017                       -0.154447   \n",
       "\n",
       "       Reads_mapped  Total_reads  \n",
       "Class                             \n",
       "1          1.491072     0.186285  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_increased.groupby('Class').mean() #1.586048 and 1.500616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2322, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_increased.shape # 2322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_increased.to_excel('data_increased.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_interpolation(data, model):\n",
    "    num_samples = data['Class'].value_counts()[0] - data['Class'].value_counts()[1]\n",
    "    X = data[data['Class'] == 1].drop(['Class'], axis=1)\n",
    "    z, _ = model.encode(X.values.astype(np.float32))\n",
    "    z1 = pd.DataFrame(z).sample(frac=num_samples / len(z), replace=True)\n",
    "    z2 = z1.sample(frac=1)\n",
    "    r = np.random.rand(*z1.shape)\n",
    "    z = r * z1.values + (1 - r) * z2.values\n",
    "    samples = model.decode(z.astype(np.float32)).numpy()\n",
    "    dfnew = pd.DataFrame(samples, columns=data.columns.drop('Class'))\n",
    "    dfnew['Class'] = np.ones(len(samples), dtype=np.int)\n",
    "    # dfnew = pd.concat((data, dfnew), ignore_index=True).sample(frac=1)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interpolated = augment_data_interpolation(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genome_length</th>\n",
       "      <th>Ratio_1kbp_bins_covered</th>\n",
       "      <th>Ratio_GC-content_uncovered_bins</th>\n",
       "      <th>Ratio_N-content_uncovered_bins</th>\n",
       "      <th>Reads_mapped</th>\n",
       "      <th>Total_reads</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276933</td>\n",
       "      <td>1.585271</td>\n",
       "      <td>-0.460965</td>\n",
       "      <td>-0.187147</td>\n",
       "      <td>1.170088</td>\n",
       "      <td>0.036149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Genome_length  Ratio_1kbp_bins_covered  \\\n",
       "Class                                           \n",
       "1           0.276933                 1.585271   \n",
       "\n",
       "       Ratio_GC-content_uncovered_bins  Ratio_N-content_uncovered_bins  \\\n",
       "Class                                                                    \n",
       "1                            -0.460965                       -0.187147   \n",
       "\n",
       "       Reads_mapped  Total_reads  \n",
       "Class                             \n",
       "1          1.170088     0.036149  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_interpolated.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genome_length</th>\n",
       "      <th>Ratio_1kbp_bins_covered</th>\n",
       "      <th>Ratio_GC-content_uncovered_bins</th>\n",
       "      <th>Ratio_N-content_uncovered_bins</th>\n",
       "      <th>Reads_mapped</th>\n",
       "      <th>Total_reads</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016082</td>\n",
       "      <td>-0.06098</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>-0.060841</td>\n",
       "      <td>-0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405132</td>\n",
       "      <td>1.53619</td>\n",
       "      <td>-0.416753</td>\n",
       "      <td>-0.077574</td>\n",
       "      <td>1.532687</td>\n",
       "      <td>0.143355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Genome_length  Ratio_1kbp_bins_covered  \\\n",
       "Class                                           \n",
       "0          -0.016082                 -0.06098   \n",
       "1           0.405132                  1.53619   \n",
       "\n",
       "       Ratio_GC-content_uncovered_bins  Ratio_N-content_uncovered_bins  \\\n",
       "Class                                                                    \n",
       "0                             0.016543                        0.003079   \n",
       "1                            -0.416753                       -0.077574   \n",
       "\n",
       "       Reads_mapped  Total_reads  \n",
       "Class                             \n",
       "0         -0.060841    -0.005691  \n",
       "1          1.532687     0.143355  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2903, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_interpolated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merging the new and the original data:\n",
    "merged_data = pd.concat((data, data_increased), ignore_index=True) # .sample(frac=1)\n",
    "\n",
    "# 2. Shuffle data \n",
    "from sklearn.utils import shuffle\n",
    "merged_data = shuffle(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genome_length</th>\n",
       "      <th>Ratio_1kbp_bins_covered</th>\n",
       "      <th>Ratio_GC-content_uncovered_bins</th>\n",
       "      <th>Ratio_N-content_uncovered_bins</th>\n",
       "      <th>Reads_mapped</th>\n",
       "      <th>Total_reads</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>-0.398347</td>\n",
       "      <td>-0.211648</td>\n",
       "      <td>-1.529774</td>\n",
       "      <td>-0.167432</td>\n",
       "      <td>-0.113029</td>\n",
       "      <td>-0.619181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>-0.197923</td>\n",
       "      <td>3.096748</td>\n",
       "      <td>-0.394659</td>\n",
       "      <td>-0.079079</td>\n",
       "      <td>2.263796</td>\n",
       "      <td>-0.118312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>-0.282902</td>\n",
       "      <td>-0.122032</td>\n",
       "      <td>-1.558441</td>\n",
       "      <td>-0.077109</td>\n",
       "      <td>-0.143124</td>\n",
       "      <td>-0.491011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>-0.793125</td>\n",
       "      <td>-0.183680</td>\n",
       "      <td>0.829575</td>\n",
       "      <td>-0.168520</td>\n",
       "      <td>-0.117813</td>\n",
       "      <td>1.944562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>-0.410265</td>\n",
       "      <td>-0.220521</td>\n",
       "      <td>1.647923</td>\n",
       "      <td>-0.168520</td>\n",
       "      <td>-0.117818</td>\n",
       "      <td>1.944562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Genome_length  Ratio_1kbp_bins_covered  Ratio_GC-content_uncovered_bins  \\\n",
       "2109      -0.398347                -0.211648                        -1.529774   \n",
       "4661      -0.197923                 3.096748                        -0.394659   \n",
       "5297      -0.282902                -0.122032                        -1.558441   \n",
       "1107      -0.793125                -0.183680                         0.829575   \n",
       "969       -0.410265                -0.220521                         1.647923   \n",
       "\n",
       "      Ratio_N-content_uncovered_bins  Reads_mapped  Total_reads  Class  \n",
       "2109                       -0.167432     -0.113029    -0.619181      0  \n",
       "4661                       -0.079079      2.263796    -0.118312      1  \n",
       "5297                       -0.077109     -0.143124    -0.491011      1  \n",
       "1107                       -0.168520     -0.117813     1.944562      0  \n",
       "969                        -0.168520     -0.117818     1.944562      0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_excel('merged_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5465, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bar plot with ggplot of SMOTE distribution \n",
    "from plotnine import *\n",
    "p = ggplot(merged_data) + geom_bar(aes(x='Class'), fill = \"yellow\", colour = \"blue\")\n",
    "p\n",
    "ggsave(plot = p, filename = 'Final_code_TFM', path = \"/Users/ferdinandosquitieri/Desktop/TFM /ML\")\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       567\n",
      "           1       0.99      1.00      0.99       643\n",
      "\n",
      "    accuracy                           0.99      1210\n",
      "   macro avg       0.99      0.99      0.99      1210\n",
      "weighted avg       0.99      0.99      0.99      1210\n",
      "\n",
      "[[561   6]\n",
      " [  3 640]]\n",
      "0.00743801652892562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1436be3d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SMOTE \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Defining X and y \n",
    "X = data.drop(['Class'], axis = 1 )\n",
    "y = data['Class']\n",
    "\n",
    "# Resampling with SMOTE algorithm \n",
    "X_smote, y_smote = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# Defining SMOTE model \n",
    "smote_model = xgb.XGBClassifier(random_state=12).fit(X_smote, y_smote) # testing on a model \n",
    "\n",
    "# Splitting data \n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X_smote, y_smote, test_size=0.2, random_state=12)\n",
    "\n",
    "# Testing the model3 with SMOTE \n",
    "smote = SMOTE(random_state=12)\n",
    "pipeline = Pipeline([('smote', smote), ('XGBClassifier', smote_model)]) # creating the pipeline we want to use  \n",
    "pipeline.fit(xtrain, ytrain) # fitting  \n",
    "pred_smote = pipeline.predict(xtest) # predicting \n",
    "print(classification_report(ytest, pred_smote)) # results\n",
    "print(confusion_matrix(ytest,pred_smote))\n",
    "print(mean_absolute_error(ytest, pred_smote))\n",
    "sns.countplot(pred_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsmote = pd.DataFrame(pred_smote, columns = ['Class'])\n",
    "predsmote.to_excel('predsmote.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predsmote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dcfe403b8f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Bar plot with ggplot of SMOTE distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotnine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mggplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredsmote\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgeom_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"yellow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predsmote' is not defined"
     ]
    }
   ],
   "source": [
    "# Bar plot with ggplot of SMOTE distribution \n",
    "from plotnine import *\n",
    "ggplot(predsmote) + geom_bar(aes(x='Class'), fill = \"yellow\", colour = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       605\n",
      "           1       0.80      0.83      0.82        24\n",
      "\n",
      "    accuracy                           0.99       629\n",
      "   macro avg       0.90      0.91      0.90       629\n",
      "weighted avg       0.99      0.99      0.99       629\n",
      "\n",
      "[[600   5]\n",
      " [  4  20]]\n",
      "0.014308426073131956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x143655040>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOXklEQVR4nO3df6zdd13H8eeLlYEi0I1e62irXaSBLFFg3MwixiCLsk2lC4EJCiuzSf1jEghGmf4hhGgCEZ0bmCUNA1qCzDnEVbKgSwGJkSG3soyxQnZdmG2zrZdtjB8LLMO3f9zP/eysu11P2b7n3O4+H8nJ+Xw+38/323eTm/PK93O+3+9JVSFJEsDTpl2AJGnlMBQkSZ2hIEnqDAVJUmcoSJK6NdMu4IlYt25dbd68edplSNJJZf/+/d+qqpnltp3UobB582bm5uamXYYknVSS3HmsbS4fSZI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoGDYUka5Ncl+TrSQ4keXmS05PcmOT29n5am5skVyaZT3JLkrOHrE2S9FhDnylcAXymql4EvBg4AFwG7KuqLcC+1gc4H9jSXjuBqwauTZJ0lMHuaE7yXOBXgbcAVNVDwENJtgGvbNN2A58H3glsA/bU4q/+3NTOMs6oqruGqhHgZX+8Z8jD6yS1/68unnYJ0lQMeaZwJrAAfCTJV5J8KMmzgPUjH/R3A+tbewNwcGT/Q23sUZLsTDKXZG5hYWHA8iVp9RkyFNYAZwNXVdVLge/zyFIRAO2s4IR+D7SqdlXVbFXNzsws+zwnSdKPachQOAQcqqovtf51LIbEPUnOAGjvR9r2w8Cmkf03tjFJ0oQMFgpVdTdwMMkL29C5wG3AXmB7G9sOXN/ae4GL21VIW4EHhv4+QZL0aEM/OvutwMeTnArcAVzCYhBdm2QHcCdwUZt7A3ABMA882OZKkiZo0FCoqpuB2WU2nbvM3AIuHbIeSdLj845mSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVI3aCgk+WaSrya5OclcGzs9yY1Jbm/vp7XxJLkyyXySW5KcPWRtkqTHmsSZwq9V1Uuqarb1LwP2VdUWYF/rA5wPbGmvncBVE6hNkjRiGstH24Ddrb0buHBkfE8tuglYm+SMKdQnSavW0KFQwL8l2Z9kZxtbX1V3tfbdwPrW3gAcHNn3UBt7lCQ7k8wlmVtYWBiqbklaldYMfPxfqarDSX4auDHJ10c3VlUlqRM5YFXtAnYBzM7OntC+kqTHN+iZQlUdbu9HgE8B5wD3LC0LtfcjbfphYNPI7hvbmCRpQgYLhSTPSvLspTbwG8CtwF5ge5u2Hbi+tfcCF7erkLYCD4wsM0mSJmDI5aP1wKeSLP07f19Vn0nyZeDaJDuAO4GL2vwbgAuAeeBB4JIBa5MkLWOwUKiqO4AXLzN+L3DuMuMFXDpUPZKk4/OOZklSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSN3goJDklyVeSfLr1z0zypSTzSf4hyalt/BmtP9+2bx66NknSo03iTOFtwIGR/vuAy6vqBcD9wI42vgO4v41f3uZJkiZo0FBIshH4TeBDrR/gVcB1bcpu4MLW3tb6tO3ntvmSpAkZ+kzhb4E/Af6v9Z8HfLuqHm79Q8CG1t4AHARo2x9o8yVJEzJYKCT5LeBIVe1/ko+7M8lckrmFhYUn89CStOoNeabwCuA1Sb4JXMPistEVwNoka9qcjcDh1j4MbAJo258L3Hv0QatqV1XNVtXszMzMgOVL0uozWChU1Z9W1caq2gy8AfhsVf0e8DngdW3aduD61t7b+rTtn62qGqo+SdJjTeM+hXcC70gyz+J3Ble38auB57XxdwCXTaE2SVrV1hx/yhNXVZ8HPt/adwDnLDPnB8DrJ1GPJGl53tEsSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktSNFQpJ9o0zJkk6uT3uU1KTPBP4SWBdktOApd9Mfg6P/IymJOkp4niPzv4D4O3A84H9PBIK3wE+OGBdkqQpeNxQqKorgCuSvLWqPjChmiRJUzLWj+xU1QeS/DKweXSfqtozUF2SpCkYKxSSfAz4eeBm4EdtuABDQZKeQsb9Oc5Z4KyqqiGLkSRN17j3KdwK/MyQhUiSpm/cM4V1wG1J/gv44dJgVb1mkKokSVMxbii8e8giJEkrw7hXH/370IVIkqZv3KuPvsvi1UYApwJPB75fVc8ZqjBJ0uSNe6bw7KV2kgDbgK1DFSVJmo4TfkpqLfpn4NUD1CNJmqJxl49eO9J9Gov3LfzgOPs8E/gC8Iz271xXVe9KciZwDfA8Fp+n9OaqeijJM1i8Ge5lwL3A71TVN0/svyNJeiLGPVP47ZHXq4HvsriE9Hh+CLyqql4MvAQ4L8lW4H3A5VX1AuB+YEebvwO4v41f3uZJkiZo3O8ULjnRA7e7n7/Xuk9vrwJeBfxuG9/N4uWuV7EYMu9u49cBH0wS76KWpMkZ90d2Nib5VJIj7fXJJBvH2O+UJDcDR4Abgf8Bvl1VD7cph3jkdxk2AAcB2vYHWFxiOvqYO5PMJZlbWFgYp3xJ0pjGXT76CLCXxd9VeD7wL23scVXVj6rqJcBG4BzgRT9mnaPH3FVVs1U1OzMz80QPJ0kaMW4ozFTVR6rq4fb6KDD2J3JVfRv4HPByYG2SpWWrjcDh1j4MbAJo25/L4hfOkqQJGTcU7k3yprYcdEqSN3GcD+wkM0nWtvZPAL8OHGAxHF7Xpm0Hrm/tva1P2/5Zv0+QpMka99lHvw98gMWrggr4T+Atx9nnDGB3klNYDJ9rq+rTSW4DrknyF8BXgKvb/KuBjyWZB+4D3nAi/xFJ0hM3bii8B9heVfcDJDkdeD+LYbGsqroFeOky43ew+P3C0eM/AF4/Zj2SpAGMu3z0i0uBAFBV97HMB74k6eQ2big8LclpS512pjDuWYYk6SQx7gf7XwNfTPKPrf964C+HKUmSNC3j3tG8J8kci3cjA7y2qm4brixJ0jSMvQTUQsAgkKSnsBN+dLYk6anLUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSd1goZBkU5LPJbktydeSvK2Nn57kxiS3t/fT2niSXJlkPsktSc4eqjZJ0vKGPFN4GPijqjoL2ApcmuQs4DJgX1VtAfa1PsD5wJb22glcNWBtkqRlDBYKVXVXVf13a38XOABsALYBu9u03cCFrb0N2FOLbgLWJjljqPokSY81ke8UkmwGXgp8CVhfVXe1TXcD61t7A3BwZLdDbezoY+1MMpdkbmFhYbCaJWk1GjwUkvwU8Eng7VX1ndFtVVVAncjxqmpXVc1W1ezMzMyTWKkkadBQSPJ0FgPh41X1T234nqVlofZ+pI0fBjaN7L6xjUmSJmTIq48CXA0cqKq/Gdm0F9je2tuB60fGL25XIW0FHhhZZpIkTcCaAY/9CuDNwFeT3NzG/gx4L3Btkh3AncBFbdsNwAXAPPAgcMmAtUmSljFYKFTVfwA5xuZzl5lfwKVD1SNJOj7vaJYkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYOFQpIPJzmS5NaRsdOT3Jjk9vZ+WhtPkiuTzCe5JcnZQ9UlSTq2Ic8UPgqcd9TYZcC+qtoC7Gt9gPOBLe21E7hqwLokSccwWChU1ReA+44a3gbsbu3dwIUj43tq0U3A2iRnDFWbJGl5k/5OYX1V3dXadwPrW3sDcHBk3qE2JkmaoKl90VxVBdSJ7pdkZ5K5JHMLCwsDVCZJq9ekQ+GepWWh9n6kjR8GNo3M29jGHqOqdlXVbFXNzszMDFqsJK02kw6FvcD21t4OXD8yfnG7Cmkr8MDIMpMkaULWDHXgJJ8AXgmsS3IIeBfwXuDaJDuAO4GL2vQbgAuAeeBB4JKh6pIkHdtgoVBVbzzGpnOXmVvApUPVIkkaj3c0S5I6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHVrpl2ApOX973t+YdolaAX62T//6qDH90xBktQZCpKkzlCQJHWGgiSpMxQkSd2KCoUk5yX5RpL5JJdNux5JWm1WTCgkOQX4O+B84CzgjUnOmm5VkrS6rJhQAM4B5qvqjqp6CLgG2DblmiRpVVlJN69tAA6O9A8Bv3T0pCQ7gZ2t+70k35hAbavFOuBb0y5iJcj7t0+7BD2af5tL3pUn4yg/d6wNKykUxlJVu4Bd067jqSjJXFXNTrsO6Wj+bU7OSlo+OgxsGulvbGOSpAlZSaHwZWBLkjOTnAq8Adg75ZokaVVZMctHVfVwkj8E/hU4BfhwVX1tymWtNi7LaaXyb3NCUlXTrkGStEKspOUjSdKUGQqSpM5QkI8X0YqV5MNJjiS5ddq1rBaGwirn40W0wn0UOG/aRawmhoJ8vIhWrKr6AnDftOtYTQwFLfd4kQ1TqkXSlBkKkqTOUJCPF5HUGQry8SKSOkNhlauqh4Glx4scAK718SJaKZJ8Avgi8MIkh5LsmHZNT3U+5kKS1HmmIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKn7fxlVh0t+FGMpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cluster Centroids \n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Splitting data \n",
    "xtrain_cc, xtest_cc, ytrain_cc, ytest_cc = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "cc = ClusterCentroids(random_state=12) # defining the method \n",
    "pipeline_cc = Pipeline([('cc', cc), ('XGBClassifier', xgb.XGBClassifier(random_state=12))]) # creating the pipeline we want to use  \n",
    "pipeline.fit(xtrain_cc, ytrain_cc) # fitting  \n",
    "pred_cc = pipeline.predict(xtest_cc) # predicting \n",
    "print(classification_report(ytest_cc, pred_cc)) # results\n",
    "print(confusion_matrix(ytest_cc,pred_cc))\n",
    "print(mean_absolute_error(ytest_cc, pred_cc))\n",
    "sns.countplot(pred_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predcentroids = pd.DataFrame(pred_cc, columns = ['Class'])\n",
    "predcentroids.to_excel('Predict_cc.xlsx')\n",
    "predcentroids[predcentroids.Class==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(predcentroids) + geom_bar(aes(x='Class'), fill = \"yellow\", colour = \"blue\")\n",
    "ggsave(plot = p, filename = 'Final_code_TFM', path = \"/Users/ferdinandosquitieri/Desktop/TFM /ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the models on the new data \n",
    "# Defining X and Y \n",
    "X_final = merged_data.drop('Class', axis=1) # data increased with VAE \n",
    "y_final = merged_data['Class']\n",
    "\n",
    "# Split the data \n",
    "xtrain_f, xtest_f, ytrain_f, ytest_f = train_test_split(X_final, y_final, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 \n",
    "from sklearn.ensemble import GradientBoostingClassifier # model 1 \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb # model 2\n",
    "from sklearn.ensemble import AdaBoostClassifier # model 3\n",
    "from sklearn.ensemble import RandomForestClassifier # model 4 \n",
    "from sklearn import svm # model 5 \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression # model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       614\n",
      "           1       0.99      0.98      0.98       479\n",
      "\n",
      "    accuracy                           0.98      1093\n",
      "   macro avg       0.98      0.98      0.98      1093\n",
      "weighted avg       0.98      0.98      0.98      1093\n",
      "\n",
      "[[608   6]\n",
      " [ 11 468]]\n",
      "0.01555352241537054\n",
      "Accuracy :  0.9844464775846294\n",
      "Sensitivity :  0.990228013029316\n",
      "Specificity :  0.9770354906054279\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=12)\n",
    "gbc.fit(xtrain_f,ytrain_f)\n",
    "predict_gbc = gbc.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, predict_gbc))\n",
    "print(confusion_matrix(ytest_f, predict_gbc))\n",
    "\n",
    "# error between data generated and original data \n",
    "print(mean_absolute_error(predict_gbc, ytest_f))\n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm1 = (confusion_matrix(ytest_f, predict_gbc)) \n",
    "total1 = sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1 = (cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])  # TN / (TN + FP)\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       614\n",
      "           1       0.99      0.99      0.99       479\n",
      "\n",
      "    accuracy                           0.99      1093\n",
      "   macro avg       0.99      0.99      0.99      1093\n",
      "weighted avg       0.99      0.99      0.99      1093\n",
      "\n",
      "[[609   5]\n",
      " [  4 475]]\n",
      "0.008234217749313814\n",
      "Accuracy :  0.9917657822506862\n",
      "Sensitivity :  0.99185667752443\n",
      "Specificity :  0.9916492693110647\n"
     ]
    }
   ],
   "source": [
    "# Model 2 \n",
    "xgb_boost = xgb.XGBClassifier(random_state=12)\n",
    "xgb_boost.fit(xtrain_f, ytrain_f)\n",
    "predict_xgb_boost = xgb_boost.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, predict_xgb_boost))\n",
    "print(confusion_matrix(ytest_f, predict_xgb_boost))\n",
    "\n",
    "# error between data generated and original data \n",
    "print(mean_absolute_error(predict_xgb_boost, ytest_f))\n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm2 = (confusion_matrix(ytest_f, predict_xgb_boost)) \n",
    "total2 = sum(sum(cm2))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy2 = (cm2[0,0]+cm2[1,1])/total2\n",
    "print ('Accuracy : ', accuracy2)\n",
    "\n",
    "sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity2 )\n",
    "\n",
    "specificity2 = cm2[1,1]/(cm2[1,0]+cm2[1,1])  # TN / (TN + FP)\n",
    "print('Specificity : ', specificity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       614\n",
      "           1       0.99      0.98      0.98       479\n",
      "\n",
      "    accuracy                           0.99      1093\n",
      "   macro avg       0.99      0.98      0.99      1093\n",
      "weighted avg       0.99      0.99      0.99      1093\n",
      "\n",
      "[[608   6]\n",
      " [ 10 469]]\n",
      "0.01463860933211345\n",
      "Accuracy :  0.9853613906678865\n",
      "Sensitivity :  0.990228013029316\n",
      "Specificity :  0.9791231732776617\n"
     ]
    }
   ],
   "source": [
    "# Model 3 \n",
    "ada = AdaBoostClassifier(random_state=12)\n",
    "ada.fit(xtrain_f,ytrain_f)\n",
    "predict_ada = ada.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, predict_ada))\n",
    "print(confusion_matrix(ytest_f, predict_ada))\n",
    "\n",
    "# error between data generated and original data \n",
    "print(mean_absolute_error(predict_ada, ytest_f))\n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm = (confusion_matrix(ytest_f, predict_ada)) \n",
    "total = sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy = (cm[0,0]+cm[1,1])/total\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0]+cm[1,1])  # TN / (TN + FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       614\n",
      "           1       0.99      0.99      0.99       479\n",
      "\n",
      "    accuracy                           0.99      1093\n",
      "   macro avg       0.99      0.99      0.99      1093\n",
      "weighted avg       0.99      0.99      0.99      1093\n",
      "\n",
      "[[611   3]\n",
      " [  4 475]]\n",
      "Accuracy :  0.9935956084172004\n",
      "Sensitivity :  0.995114006514658\n",
      "Specificity :  0.9916492693110647\n"
     ]
    }
   ],
   "source": [
    "# Model 4 \n",
    "rfc = RandomForestClassifier(n_estimators = 200, random_state = 12)\n",
    "rfc.fit(xtrain_f, ytrain_f)\n",
    "pred_rfc = rfc.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, pred_rfc))\n",
    "print(confusion_matrix(ytest_f, pred_rfc))\n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm1 = (confusion_matrix(ytest_f, pred_rfc)) \n",
    "total1 = sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1 = (cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])  # TN / (TN + FP)\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       614\n",
      "           1       0.90      0.84      0.87       479\n",
      "\n",
      "    accuracy                           0.89      1093\n",
      "   macro avg       0.89      0.88      0.88      1093\n",
      "weighted avg       0.89      0.89      0.89      1093\n",
      "\n",
      "[[567  47]\n",
      " [ 76 403]]\n",
      "Accuracy :  0.8874656907593779\n",
      "Sensitivity :  0.9234527687296417\n",
      "Specificity :  0.8413361169102297\n"
     ]
    }
   ],
   "source": [
    "# Model 5 \n",
    "SVM = svm.SVC(random_state = 12)\n",
    "SVM.fit(xtrain_f, ytrain_f)\n",
    "pred_SVM = SVM.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, pred_SVM)) \n",
    "print(confusion_matrix(ytest_f, pred_SVM)) \n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm2 = (confusion_matrix(ytest_f, pred_SVM)) \n",
    "total2=sum(sum(cm2))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy2=(cm2[0,0]+cm2[1,1])/total2\n",
    "print ('Accuracy : ', accuracy2)\n",
    "\n",
    "sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity2 )\n",
    "\n",
    "specificity2 = cm2[1,1]/(cm2[1,0]+cm2[1,1]) # TN / (TN + FP)\n",
    "print('Specificity : ', specificity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       614\n",
      "           1       0.87      0.74      0.80       479\n",
      "\n",
      "    accuracy                           0.84      1093\n",
      "   macro avg       0.84      0.83      0.83      1093\n",
      "weighted avg       0.84      0.84      0.84      1093\n",
      "\n",
      "[[561  53]\n",
      " [124 355]]\n",
      "Accuracy :  0.8380603842634949\n",
      "Sensitivity :  0.9136807817589576\n",
      "Specificity :  0.7411273486430062\n"
     ]
    }
   ],
   "source": [
    "# Model 6 \n",
    "logreg = LogisticRegression(solver='lbfgs', random_state = 12)\n",
    "logreg.fit(xtrain_f, ytrain_f)\n",
    "pred_logreg = logreg.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, pred_logreg)) \n",
    "print(confusion_matrix(ytest_f, pred_logreg)) \n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm3 = (confusion_matrix(ytest_f, pred_logreg)) \n",
    "total3=sum(sum(cm3))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy3=(cm3[0,0]+cm3[1,1])/total3\n",
    "print ('Accuracy : ', accuracy3)\n",
    "\n",
    "sensitivity3 = cm3[0,0]/(cm3[0,0]+cm3[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity3 )\n",
    "\n",
    "specificity3 = cm3[1,1]/(cm3[1,0]+cm3[1,1]) # TN / (TN + FP)\n",
    "print('Specificity : ', specificity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': 12, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning - Model 1 \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV # Grid Search \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "print(gbc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = [{'ccp_alpha' : [0.0,0.5,0.7], 'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators':[100,200,400,600,800] }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_ccp_alpha</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720062</td>\n",
       "      <td>0.049945</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.457903</td>\n",
       "      <td>0.040496</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>200</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...</td>\n",
       "      <td>0.743494</td>\n",
       "      <td>0.698620</td>\n",
       "      <td>0.705219</td>\n",
       "      <td>0.681174</td>\n",
       "      <td>0.712117</td>\n",
       "      <td>0.708125</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.211356</td>\n",
       "      <td>0.324998</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>400</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...</td>\n",
       "      <td>0.928565</td>\n",
       "      <td>0.901622</td>\n",
       "      <td>0.910855</td>\n",
       "      <td>0.881075</td>\n",
       "      <td>0.889028</td>\n",
       "      <td>0.902229</td>\n",
       "      <td>0.016674</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.558359</td>\n",
       "      <td>0.356853</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>600</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...</td>\n",
       "      <td>0.928565</td>\n",
       "      <td>0.910710</td>\n",
       "      <td>0.915867</td>\n",
       "      <td>0.893463</td>\n",
       "      <td>0.898106</td>\n",
       "      <td>0.909342</td>\n",
       "      <td>0.012589</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.018112</td>\n",
       "      <td>0.269973</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>800</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...</td>\n",
       "      <td>0.928565</td>\n",
       "      <td>0.910710</td>\n",
       "      <td>0.915867</td>\n",
       "      <td>0.893463</td>\n",
       "      <td>0.898106</td>\n",
       "      <td>0.909342</td>\n",
       "      <td>0.012589</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.688310</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.314400</td>\n",
       "      <td>0.087770</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3.410891</td>\n",
       "      <td>0.524735</td>\n",
       "      <td>0.010030</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>400</td>\n",
       "      <td>{'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6.236772</td>\n",
       "      <td>1.037393</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>600</td>\n",
       "      <td>{'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>6.527754</td>\n",
       "      <td>0.957607</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>800</td>\n",
       "      <td>{'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336763</td>\n",
       "      <td>0.336807</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.720062      0.049945         0.007997        0.001962   \n",
       "1        1.457903      0.040496         0.008410        0.001611   \n",
       "2        3.211356      0.324998         0.011052        0.001091   \n",
       "3        4.558359      0.356853         0.013491        0.001891   \n",
       "4        6.018112      0.269973         0.015880        0.002042   \n",
       "..            ...           ...              ...             ...   \n",
       "85       0.688310      0.020503         0.005589        0.000582   \n",
       "86       1.314400      0.087770         0.005149        0.000485   \n",
       "87       3.410891      0.524735         0.010030        0.007068   \n",
       "88       6.236772      1.037393         0.007473        0.001439   \n",
       "89       6.527754      0.957607         0.006748        0.002490   \n",
       "\n",
       "   param_ccp_alpha param_learning_rate param_n_estimators  \\\n",
       "0                0              0.0001                100   \n",
       "1                0              0.0001                200   \n",
       "2                0              0.0001                400   \n",
       "3                0              0.0001                600   \n",
       "4                0              0.0001                800   \n",
       "..             ...                 ...                ...   \n",
       "85             0.7                 0.3                100   \n",
       "86             0.7                 0.3                200   \n",
       "87             0.7                 0.3                400   \n",
       "88             0.7                 0.3                600   \n",
       "89             0.7                 0.3                800   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...           0.336986   \n",
       "1   {'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...           0.743494   \n",
       "2   {'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...           0.928565   \n",
       "3   {'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...           0.928565   \n",
       "4   {'ccp_alpha': 0.0, 'learning_rate': 0.0001, 'n...           0.928565   \n",
       "..                                                ...                ...   \n",
       "85  {'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...           0.336986   \n",
       "86  {'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...           0.336986   \n",
       "87  {'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...           0.336986   \n",
       "88  {'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...           0.336986   \n",
       "89  {'ccp_alpha': 0.7, 'learning_rate': 0.3, 'n_es...           0.336986   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.336763           0.336763           0.336763   \n",
       "1            0.698620           0.705219           0.681174   \n",
       "2            0.901622           0.910855           0.881075   \n",
       "3            0.910710           0.915867           0.893463   \n",
       "4            0.910710           0.915867           0.893463   \n",
       "..                ...                ...                ...   \n",
       "85           0.336763           0.336763           0.336763   \n",
       "86           0.336763           0.336763           0.336763   \n",
       "87           0.336763           0.336763           0.336763   \n",
       "88           0.336763           0.336763           0.336763   \n",
       "89           0.336763           0.336763           0.336763   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.336763         0.336807        0.000089               30  \n",
       "1            0.712117         0.708125        0.020455               29  \n",
       "2            0.889028         0.902229        0.016674               28  \n",
       "3            0.898106         0.909342        0.012589               26  \n",
       "4            0.898106         0.909342        0.012589               26  \n",
       "..                ...              ...             ...              ...  \n",
       "85           0.336763         0.336807        0.000089               30  \n",
       "86           0.336763         0.336807        0.000089               30  \n",
       "87           0.336763         0.336807        0.000089               30  \n",
       "88           0.336763         0.336807        0.000089               30  \n",
       "89           0.336763         0.336807        0.000089               30  \n",
       "\n",
       "[90 rows x 16 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "tuned_gbc = GridSearchCV(GradientBoostingClassifier(), tuned_params, cv = 5, scoring = 'f1_macro')\n",
    "tuned_gbc.fit(xtrain, ytrain)\n",
    "tuned_gbc.predict(xtest)\n",
    "tuned_gbc.cv_results_\n",
    "\n",
    "gbc_table = pd.DataFrame(tuned_gbc.cv_results_)\n",
    "gbc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_gbc.best_score_ # 0.9890390787112274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 \n",
    "print('Parameters currently in use:\\n')\n",
    "print(xgb_boost.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params_xgb = [{'base_score' : [0.0,0.5,0.7], 'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators':[100,200,400,600,800] }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "tuned_xgb = GridSearchCV(xgb.XGBClassifier(random_state=12), tuned_params, cv = 5, scoring = 'f1_macro')\n",
    "tuned_xgb.fit(xtrain, ytrain)\n",
    "tuned_xgb.cv_results_\n",
    "\n",
    "xgb_table = pd.DataFrame(tuned_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 \n",
    "print('Parameters currently in use:\\n')\n",
    "print(ada.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_ada_params = [{'algorithm' : ['SAMME', 'SAMME.R'], 'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators':[100,200,400,600,800] }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "tuned_ada= GridSearchCV(AdaBoostClassifier(random_state=12), tuned_ada_params, cv = 5, scoring = 'f1_macro')\n",
    "tuned_ada.fit(xtrain, ytrain)\n",
    "tuned_ada.cv_results_\n",
    "\n",
    "ada_table = pd.DataFrame(tuned_ada.cv_results_)\n",
    "ada_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 \n",
    "print('Parameters currently in use:\\n')\n",
    "print(rfc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params_rfc = [{'n_estimators' : [100,500,800], 'oob_score' : [False, True], 'ccp_alpha' : [0.0, 0.5, 0.7], 'criterion' : ['gini', 'entropy']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rfc = GridSearchCV(RandomForestClassifier(random_state = 12), tuned_params_rfc, cv = 5, scoring = 'f1_macro')\n",
    "tuned_rfc.fit(xtrain, ytrain)\n",
    "tuned_rfc.cv_results_\n",
    "\n",
    "rfc_table = pd.DataFrame(tuned_rfc.cv_results_)\n",
    "rfc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 \n",
    "print('Parameters currently in use:\\n')\n",
    "print(SVM.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_svm = [{'kernel' : ['rbf'], 'gamma' : [1e-3, 1e-4], \n",
    "                    'C' : [1,10,100,1000]},\n",
    "                   {'kernel' : ['linear'], 'C' : [1,10,100,1000]}]\n",
    "\n",
    "np.random.seed(123)\n",
    "tuned_svm = GridSearchCV(svm.SVC(), tuned_svm, cv = 5, scoring = 'f1_macro')\n",
    "tuned_svm.fit(xtrain,ytrain) #testing \n",
    "tuned_svm.best_params_\n",
    "tuned_svm.cv_results_\n",
    "\n",
    "svm_table = pd.DataFrame(tuned_svm.cv_results_)\n",
    "svm_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6 \n",
    "print('Parameters currently in use:\\n')\n",
    "print(logreg.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_logreg = [{'penalty' : ['l5', 'l1', 'l2', 'l20'],  \n",
    "                    'C' : [1,10,100,1000],\n",
    "               'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "tuned_logreg = GridSearchCV(LogisticRegression(), tuned_logreg, cv = 5, scoring = 'f1_macro')\n",
    "tuned_logreg.fit(xtrain,ytrain) #testing \n",
    "tuned_logreg.cv_results_\n",
    "\n",
    "logreg_table = pd.DataFrame(tuned_logreg.cv_results_)\n",
    "logreg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'SVM' : {\n",
    "        'Model': svm.SVC(random_state = 1),\n",
    "        'Params': {\n",
    "            'C' : [1,10,100,1000],\n",
    "            'gamma' : [1e-3, 1e-4], \n",
    "        }\n",
    "    },\n",
    "    'Random_forest' : {\n",
    "        'Model': RandomForestClassifier(random_state = 1),\n",
    "        'Params': {\n",
    "            'n_estimators' : [100,500,800],\n",
    "            'oob_score' : [False, True]\n",
    "        }\n",
    "    },\n",
    "    'Logistic_regression' : {\n",
    "        'Model': LogisticRegression(random_state = 1),\n",
    "        'Params': {\n",
    "        'C' : [1,10,100,1000],\n",
    "            'penalty' : ['l1', 'l2'],\n",
    "            'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "},\n",
    "    'Gradient_Boosting' : {\n",
    "        'Model' : GradientBoostingClassifier(random_state = 1),\n",
    "        'Params' : {\n",
    "            'ccp_alpha' : [0.0,0.5,0.7], \n",
    "            'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], \n",
    "            'n_estimators':[100,200,400,600,800]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'XGB_Boosting' : {\n",
    "        'Model' : xgb.XGBClassifier(random_state = 1),\n",
    "        'Params' : {\n",
    "            'base_score' : [0.0,0.5,0.7], \n",
    "            'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], \n",
    "            'n_estimators':[100,200,400,600,800]  \n",
    "        }\n",
    "    }, \n",
    "    \n",
    "    'Ada_Boosting' : {\n",
    "        'Model' : AdaBoostClassifier(random_state = 1),\n",
    "        'Params' : {\n",
    "            'algorithm' : ['SAMME', 'SAMME.R'], \n",
    "            'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], \n",
    "            'n_estimators':[100,200,400,600,800]  \n",
    "        }\n",
    "    } \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "np.random.seed(12)\n",
    "for model_name, mp in model_params.items():\n",
    "    final_model = GridSearchCV(mp['Model'], mp['Params'], cv=5, return_train_score=False, scoring='f1_micro')\n",
    "    final_model.fit(X_final,y_final)\n",
    "    scores.append({\n",
    "        'Model': model_name,\n",
    "        'Best score' : final_model.best_score_,\n",
    "        'Best params': final_model.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params_table = pd.DataFrame(scores,columns=['Model', 'Best score', 'Best params']).sort_values(by=['Best score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params_table.iloc[1,2] # to check the best parameters of the XGB_Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(base_score = 0.7, learning_rate = 0.3, n_estimators = 800, random_state=12)\n",
    "final_model.fit(xtrain_f, ytrain_f)\n",
    "final_predict = final_model.predict(xtest_f)\n",
    "\n",
    "print(classification_report(ytest_f, final_predict))\n",
    "print(confusion_matrix(ytest_f, final_predict))\n",
    "\n",
    "# Calculating Sensitivity and Specificity from confusion matrix : \n",
    "cm2 = (confusion_matrix(ytest_f, final_predict)) \n",
    "total2 = sum(sum(cm2))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy2 = (cm2[0,0]+cm2[1,1])/total2\n",
    "print ('Accuracy : ', accuracy2)\n",
    "\n",
    "sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[0,1]) #  TP / (TP+FN)\n",
    "print('Sensitivity : ', sensitivity2 )\n",
    "\n",
    "specificity2 = cm2[1,1]/(cm2[1,0]+cm2[1,1])  # TN / (TN + FP)\n",
    "print('Specificity : ', specificity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c84f1101b3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtitles_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "titles_options = [('Final Model')]\n",
    "for title in titles_options:\n",
    "    disp = plot_confusion_matrix(final_model, xtest_f, ytest_f)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(final_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pickle object of the final model : \n",
    "import pickle    \n",
    "\n",
    "#save the model\n",
    "pickle.dump(final_model, open('final_model.sav', 'wb'))\n",
    "# save the scaler\n",
    "pickle.dump(sc, open('scaler.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
